{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b69048-cbaf-420c-b3a3-b2a1bf292808",
   "metadata": {},
   "source": [
    "Q1 . ans \n",
    "\n",
    "Activation functions introduce non-linearity into the network. Without non-linearity, a neural network would be equivalent to a linear model, and it would not be able to model complex patterns in data. Activation functions also apply a threshold to the input value, and if the input surpasses the threshold, the neuron becomes activated (fires), allowing information to pass to the next layer. The thresholding behavior introduces the non-linearity into the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7523b118-5f01-47b3-b3fe-8c9b3e1fe595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eed7034c-ab69-4d8d-b80c-ad03e02080f7",
   "metadata": {},
   "source": [
    "Q2 . ans\n",
    "\n",
    "There are several common types of activation functions used in neural networks, each with its own characteristics and use cases. few of them are \n",
    "Relu , softmax , tanh , sigmoid , leakyrelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02cc81d-e29b-4656-bd32-45b61aed069f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b550544c-ece4-450d-8b33-42bf26eee246",
   "metadata": {},
   "source": [
    "Q3. ans\n",
    "\n",
    "They affect both the network's ability to learn complex patterns and the stability of the training process. Activation functions introduce non-linearity into the network. This non-linearity is important for the network's ability to approximate complex, non-linear functions in data. Without activation functions, the entire neural network would behave like a linear model. Activation functions affect the flow of gradients during backpropagation, which is crucial for updating the network's weights. The choice of activation function can impact whether gradients are vanishing (becoming very small) or exploding (becoming very large) during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c61530-4af1-45c7-b01a-81a8b778345d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e95fd2ab-5bc3-433c-abc5-c6efa4c7c347",
   "metadata": {},
   "source": [
    "Q4 . ans\n",
    "\n",
    "The sigmoid activation function, also known as the logistic activation function, is a widely used activation function in artificial neural networks. It maps input values to a range between 0 and 1\n",
    "- work\n",
    "\n",
    "The sigmoid function takes any real-valued number as input.\n",
    "It applies the exponential function to which makes it positive and tends to zero for large negative values of\n",
    "It adds 1 to the result, ensuring that the denominator is always positive.\n",
    "Then, it takes the reciprocal of this value, which maps the result to the range (0, 1).\n",
    "\n",
    "- Advantage of the Sigmoid\n",
    "The primary advantage of the sigmoid function is that it squashes input values to the range between 0 and 1. This makes it suitable for binary classification problems where the output represents a probability-like value.\n",
    "The sigmoid function is smooth and differentiable everywhere, which makes it well-suited for gradient-based optimization algorithms\n",
    "\n",
    "- Disadvantage of the Sigmoid\n",
    "One of the disadvantages of the sigmoid function is the vanishing gradient problem. As the input moves away from zero (either positively or negatively), the gradient of the sigmoid function becomes very close to zero. This means that during backpropagation, gradients can become extremely small, making it challenging to update the weights of layers far from the output. It can slow down or hinder the training of deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc15027-97f6-48c2-8fca-fefeb0e8a82f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68c15118-9bfb-4fd4-ba4e-2130ed20bcc0",
   "metadata": {},
   "source": [
    "Q5 . ans\n",
    "\n",
    "The Rectified Linear Unit (ReLU) activation function is a popular non-linear activation function used in artificial neural networks. It differs significantly from the sigmoid function in terms of its mathematical form and properties. sigmoid is a non linear function while relu is a linear function\n",
    "- Relu function =\n",
    "ReLU(x)=max(0,x)\n",
    "\n",
    "ReLU function has an output range of [0, âˆž). It returns the input value for positive inputs and zero for negative inputs. Sigmoid The sigmoid function maps inputs to the range (0, 1), resulting in a sigmoid-shaped curve. It returns values close to 1 for positive inputs and close to 0 for negative inputs. \n",
    "\n",
    "ReLU is not zero-centered, meaning it does not have a mean close to zero.\n",
    "Sigmoid is not zero-centered either; it has a mean close to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ac2f9-e751-468e-a6ec-e5dd2804640a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bf36555-a402-4ec8-b8b8-f80b5adf3dda",
   "metadata": {},
   "source": [
    "Q6 . ans\n",
    "\n",
    "ReLU addresses the vanishing gradient problem more effectively than the sigmoid function. In sigmoid, as the input moves away from zero (either positively or negatively), the gradients become very close to zero, making it challenging to train deep networks. ReLU, on the other hand, has gradients of 1 for positive inputs, allowing gradients to flow more freely during backpropagation and mitigating the vanishing gradient problem. ReLU can lead to faster convergence during training. Its non-linearity allows for faster learning in the early stages of training, which can help reduce training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57db196-e965-4af6-97e8-0da6b6072ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4be7dc1c-f34b-4b28-ae7a-1834bb6cc290",
   "metadata": {},
   "source": [
    "Q7 . ans\n",
    "\n",
    "Leaky Rectified Linear Unit (Leaky ReLU) is an activation function that is introduced to address some of the limitations of the standard Rectified Linear Unit (ReLU) activation function, particularly the \"dying ReLU\" problem. The \"dying ReLU\" problem occurs when neurons using ReLU become inactive during training, producing zero gradients and effectively stopping learning. The key difference between Leaky ReLU and standard ReLU is that Leaky ReLU introduces a small, non-zero slope alpha for negative inputs. This means that even when the input is negative, the activation is not completely zeroed out, as it is in standard ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e03e2-f600-4e0a-843c-8a7490eaa01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9c896e5-2b5e-4230-aa0f-f6632279e429",
   "metadata": {},
   "source": [
    "Q8 . ans\n",
    "\n",
    "The softmax activation function is a commonly used activation function in neural networks, particularly in the context of multiclass classification problems. Its primary purpose is to convert a vector of raw, unnormalized scores into a probability distribution over multiple classes. \n",
    "\n",
    "softmax activation is commonly used in the output layer of neural networks for multiclass classification problems. In that problems, the goal is to assign an input to one of multiple possible classes. The softmax function converts the raw model outputs into class probabilities, allowing the network to make class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6ba7d-a21d-4d67-968d-e89fdf3b2549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cfc7401-e665-4e54-9f84-f7856417d6b3",
   "metadata": {},
   "source": [
    "Q9. ans\n",
    "\n",
    "\n",
    "The hyperbolic tangent (tanh) activation function is a non-linear activation function that is often used in artificial neural networks. It is similar in some ways to the sigmoid activation function but has a different range \n",
    "\n",
    "- Tanh: The tanh function maps input values to the range between -1 and 1, makes it is zero-centered. It produces negative values for negative inputs and positive values for positive inputs.\n",
    "\n",
    "- Sigmoid: The sigmoid function maps input values to the range between 0 and 1, makes it non-zero-centered. It produces values close to 0 for negative inputs and values close to 1 for positive inputs.\n",
    "\n",
    "- Both tanh and sigmoid introduce non-linearity into the network, which allows neural networks to model complex, non-linear relationships in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca6dbc-875b-4f93-850a-937abe4f8e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
